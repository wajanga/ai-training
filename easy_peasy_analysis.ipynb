{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "532e4cd3",
   "metadata": {},
   "source": [
    "\n",
    "# End-to-End ML (Kaggle Dataset)\n",
    "\n",
    "This notebook shows how to go from **business question → production approach** using a Kaggle dataset.\n",
    "We will predict claim outcomes and/or flag high-cost claims, following a practical ML workflow:\n",
    "\n",
    "1. Frame the business & ML problem  \n",
    "2. Load the data  \n",
    "3. Explore & visualize  \n",
    "4. Prepare & engineer features  \n",
    "5. Train baseline models (regression & classification)  \n",
    "6. Fine-tune with cross-validation  \n",
    "7. Present results (metrics, feature importances, ROC)  \n",
    "8. Discuss deployment: launch, monitor, maintain\n",
    "\n",
    "> You can upload the **full Kaggle CSV**. For quick testing, a small sample CSV is included.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e595f9ea",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Look at the Big Picture — Frame the Problem\n",
    "\n",
    "**Business questions (examples):**\n",
    "- *Regression:* What will the **ultimate incurred claim cost** be?  \n",
    "- *Classification:* Will the claim be **high-cost** (e.g., top 10%)?\n",
    "\n",
    "**Why it matters:**\n",
    "- Better reserving and budgeting.\n",
    "- Early triage and investigation of risky claims.\n",
    "- Faster processing of low-risk claims.\n",
    "\n",
    "**ML setup:**\n",
    "- Supervised learning on historical claims.\n",
    "- Target: `UltimateIncurredClaimCost` (regression) or `HighCost` (classification).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3712c56",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Get the Data — Load the CSV\n",
    "\n",
    "- **Option A:** Upload your Kaggle CSV in the left Files pane, then set `csv_path` below.\n",
    "- **Option B:** Use the included mini sample to verify the pipeline works end-to-end.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e15cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Set to your uploaded Kaggle CSV file name, e.g.:\n",
    "# csv_path = \"/content/<your_kaggle_file>.csv\"\n",
    "# For demo, we'll start with the small sample:\n",
    "csv_path = \"datasets/easy_peasy.csv\"\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce98acf",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Explore & Visualize\n",
    "We'll parse dates, engineer simple time features, and look at distributions and relationships.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a7d7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Parse ISO timestamps\n",
    "for col in [\"DateTimeOfAccident\", \"DateReported\"]:\n",
    "    df[col] = pd.to_datetime(df[col], errors=\"coerce\")\n",
    "\n",
    "# Feature: Days between accident and report\n",
    "df[\"DaysToReport\"] = (df[\"DateReported\"] - df[\"DateTimeOfAccident\"]).dt.days\n",
    "\n",
    "# Time features\n",
    "df[\"AccidentYear\"] = df[\"DateTimeOfAccident\"].dt.year\n",
    "df[\"AccidentMonth\"] = df[\"DateTimeOfAccident\"].dt.month\n",
    "\n",
    "df[[\"DateTimeOfAccident\",\"DateReported\",\"DaysToReport\",\"AccidentYear\",\"AccidentMonth\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc7a43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ece5fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Target distribution (if available)\n",
    "if \"UltimateIncurredClaimCost\" in df.columns:\n",
    "    plt.figure()\n",
    "    df[\"UltimateIncurredClaimCost\"].hist(bins=40)\n",
    "    plt.xlabel(\"UltimateIncurredClaimCost\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.title(\"Target Distribution (Raw)\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure()\n",
    "    import numpy as np\n",
    "    np.log1p(df[\"UltimateIncurredClaimCost\"]).hist(bins=40)\n",
    "    plt.xlabel(\"log1p(UltimateIncurredClaimCost)\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.title(\"Target Distribution (Log)\")\n",
    "    plt.show()\n",
    "\n",
    "# Relationship: WeeklyWages vs Ultimate Cost\n",
    "if {\"WeeklyWages\",\"UltimateIncurredClaimCost\"} <= set(df.columns):\n",
    "    plt.figure()\n",
    "    plt.scatter(df[\"WeeklyWages\"], df[\"UltimateIncurredClaimCost\"])\n",
    "    plt.xlabel(\"WeeklyWages\")\n",
    "    plt.ylabel(\"UltimateIncurredClaimCost\")\n",
    "    plt.title(\"WeeklyWages vs Ultimate Cost\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a960950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation (numerical columns only)\n",
    "num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "if \"UltimateIncurredClaimCost\" in df.columns:\n",
    "    corr = df[num_cols].corr(numeric_only=True)\n",
    "    corr_top = corr[\"UltimateIncurredClaimCost\"].sort_values(ascending=False).head(10)\n",
    "    display(corr_top.to_frame(\"corr_with_target\"))\n",
    "else:\n",
    "    print(\"UltimateIncurredClaimCost not found; skipping correlation view.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fc7ff3",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Prepare the Data\n",
    "- Select features and target.\n",
    "- Handle categorical variables via one-hot encoding.\n",
    "- Scale numerical features for linear models.\n",
    "- Split into train/test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5eb6793",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Classification target (high cost = top 10%)\n",
    "if \"UltimateIncurredClaimCost\" in df.columns:\n",
    "    threshold = df[\"UltimateIncurredClaimCost\"].quantile(0.9)\n",
    "    df[\"HighCost\"] = (df[\"UltimateIncurredClaimCost\"] >= threshold).astype(int)\n",
    "\n",
    "drop_cols = [\"ClaimNumber\",\"DateTimeOfAccident\",\"DateReported\"]\n",
    "target_reg = \"UltimateIncurredClaimCost\"\n",
    "target_clf = \"HighCost\" if \"HighCost\" in df.columns else None\n",
    "\n",
    "feature_df = df.drop(columns=[c for c in drop_cols if c in df.columns])\n",
    "X = feature_df.drop(columns=[t for t in [target_reg, target_clf] if t in feature_df.columns])\n",
    "y_reg = df[target_reg] if target_reg in df.columns else None\n",
    "y_clf = df[target_clf] if target_clf else None\n",
    "\n",
    "cat_cols = X.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "num_cols = X.select_dtypes(exclude=[\"object\"]).columns.tolist()\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), num_cols),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
    "    ]\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train_reg, y_test_reg = train_test_split(X, y_reg, test_size=0.2, random_state=42)\n",
    "if y_clf is not None:\n",
    "    _, _, y_train_clf, y_test_clf = train_test_split(X, y_clf, test_size=0.2, random_state=42)\n",
    "\n",
    "len(X_train), len(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e118a89e",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Select & Train Baseline Models\n",
    "- **Regression:** Linear Regression and Random Forest Regressor\n",
    "- **Classification:** Logistic Regression and Random Forest Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c08493",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, classification_report, roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Regression baselines\n",
    "reg_lin = Pipeline([(\"prep\", preprocessor), (\"model\", LinearRegression())])\n",
    "reg_lin.fit(X_train, y_train_reg)\n",
    "pred_lin = reg_lin.predict(X_test)\n",
    "print(\"Regression — Linear: RMSE={:.2f}, MAE={:.2f}, R2={:.3f}\".format(\n",
    "    np.sqrt(mean_squared_error(y_test_reg, pred_lin)),\n",
    "    mean_absolute_error(y_test_reg, pred_lin),\n",
    "    r2_score(y_test_reg, pred_lin)\n",
    "))\n",
    "\n",
    "reg_rf = Pipeline([(\"prep\", preprocessor), (\"model\", RandomForestRegressor(random_state=42))])\n",
    "reg_rf.fit(X_train, y_train_reg)\n",
    "pred_rf = reg_rf.predict(X_test)\n",
    "print(\"Regression — RF:     RMSE={:.2f}, MAE={:.2f}, R2={:.3f}\".format(\n",
    "    np.sqrt(mean_squared_error(y_test_reg, pred_rf)),\n",
    "    mean_absolute_error(y_test_reg, pred_rf),\n",
    "    r2_score(y_test_reg, pred_rf)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c35b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification baselines\n",
    "if \"HighCost\" in df.columns and df[\"HighCost\"].nunique() == 2:\n",
    "    clf_log = Pipeline([(\"prep\", preprocessor), (\"model\", LogisticRegression(max_iter=200))])\n",
    "    clf_log.fit(X_train, y_train_clf)\n",
    "    proba_log = clf_log.predict_proba(X_test)[:,1]\n",
    "    pred_log = (proba_log >= 0.5).astype(int)\n",
    "    print(\"Classification — Logistic AUC: {:.3f}\".format(roc_auc_score(y_test_clf, proba_log)))\n",
    "    print(classification_report(y_test_clf, pred_log))\n",
    "\n",
    "    clf_rf = Pipeline([(\"prep\", preprocessor), (\"model\", RandomForestClassifier(random_state=42))])\n",
    "    clf_rf.fit(X_train, y_train_clf)\n",
    "    proba_rf = clf_rf.predict_proba(X_test)[:,1]\n",
    "    pred_rf = (proba_rf >= 0.5).astype(int)\n",
    "    print(\"Classification — RF AUC: {:.3f}\".format(roc_auc_score(y_test_clf, proba_rf)))\n",
    "    print(classification_report(y_test_clf, pred_rf))\n",
    "\n",
    "    # ROC curves\n",
    "    plt.figure()\n",
    "    fpr_log, tpr_log, _ = roc_curve(y_test_clf, proba_log)\n",
    "    fpr_rf, tpr_rf, _ = roc_curve(y_test_clf, proba_rf)\n",
    "    plt.plot(fpr_log, tpr_log, label=\"Logistic\")\n",
    "    plt.plot(fpr_rf, tpr_rf, label=\"RandomForest\")\n",
    "    plt.plot([0,1],[0,1], linestyle=\"--\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"ROC Curves\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Classification target not available; skipping classification section.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa49d196",
   "metadata": {},
   "source": [
    "\n",
    "## 6) Fine-Tune Your Model (Randomized Search)\n",
    "Small search for Random Forest — both regression and classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c09161",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "rf_reg = Pipeline([(\"prep\", preprocessor), (\"model\", RandomForestRegressor(random_state=42))])\n",
    "param_dist_reg = {\n",
    "    \"model__n_estimators\": [100, 200, 400],\n",
    "    \"model__max_depth\": [None, 10, 20, 40],\n",
    "    \"model__min_samples_split\": [2, 5, 10]\n",
    "}\n",
    "# Use neg_mean_squared_error for broad version compatibility; we'll report RMSE on the test set below.\n",
    "search_reg = RandomizedSearchCV(\n",
    "    rf_reg,\n",
    "    param_distributions=param_dist_reg,\n",
    "    n_iter=6,\n",
    "    scoring=\"neg_mean_squared_error\",\n",
    "    cv=3,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "search_reg.fit(X_train, y_train_reg)\n",
    "best_reg = search_reg.best_estimator_\n",
    "print(\"Best reg params:\", search_reg.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a241986",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "pred_best = best_reg.predict(X_test)\n",
    "print(\"Best RF Test RMSE:\", np.sqrt(mean_squared_error(y_test_reg, pred_best)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7161ba4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"HighCost\" in df.columns and df[\"HighCost\"].nunique() == 2:\n",
    "    rf_clf = Pipeline([(\"prep\", preprocessor), (\"model\", RandomForestClassifier(random_state=42))])\n",
    "    param_dist_clf = {\n",
    "        \"model__n_estimators\": [100, 200, 400],\n",
    "        \"model__max_depth\": [None, 10, 20, 40],\n",
    "        \"model__min_samples_split\": [2, 5, 10]\n",
    "    }\n",
    "    search_clf = RandomizedSearchCV(rf_clf, param_distributions=param_dist_clf, n_iter=6,\n",
    "                                    scoring=\"roc_auc\", cv=3, random_state=42, n_jobs=-1)\n",
    "    search_clf.fit(X_train, y_train_clf)\n",
    "    best_clf = search_clf.best_estimator_\n",
    "    print(\"Best clf params:\", search_clf.best_params_)\n",
    "\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    proba_best = best_clf.predict_proba(X_test)[:,1]\n",
    "    print(\"Best RF Test AUC:\", roc_auc_score(y_test_clf, proba_best))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733a8246",
   "metadata": {},
   "source": [
    "\n",
    "## 7) Present Your Solution — Drivers & Insights\n",
    "We show feature importances (top 10) for tuned models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366fbd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_feature_names(prep, num_cols, cat_cols):\n",
    "    num_features = list(num_cols)\n",
    "    ohe = None\n",
    "    try:\n",
    "        ohe = prep.named_transformers_[\"cat\"]\n",
    "    except Exception:\n",
    "        for name, trans, cols in prep.transformers_:\n",
    "            if name == \"cat\":\n",
    "                ohe = trans\n",
    "                break\n",
    "    if hasattr(ohe, \"get_feature_names_out\"):\n",
    "        cat_features = list(ohe.get_feature_names_out(cat_cols))\n",
    "    else:\n",
    "        cat_features = []\n",
    "    return num_features + cat_features\n",
    "\n",
    "num_cols = X.select_dtypes(exclude=[\"object\"]).columns.tolist()\n",
    "cat_cols = X.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "\n",
    "# Regression importances\n",
    "feature_names = get_feature_names(best_reg.named_steps[\"prep\"], num_cols, cat_cols)\n",
    "importances = best_reg.named_steps[\"model\"].feature_importances_\n",
    "order = np.argsort(importances)[::-1][:10]\n",
    "\n",
    "plt.figure()\n",
    "plt.barh([feature_names[i] for i in order][::-1], importances[order][::-1])\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.title(\"Top 10 Features (Regression RF)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deaf6687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification importances\n",
    "if \"HighCost\" in df.columns and df[\"HighCost\"].nunique() == 2:\n",
    "    feature_names = get_feature_names(best_clf.named_steps[\"prep\"], num_cols, cat_cols)\n",
    "    importances = best_clf.named_steps[\"model\"].feature_importances_\n",
    "    order = np.argsort(importances)[::-1][:10]\n",
    "\n",
    "    plt.figure()\n",
    "    plt.barh([feature_names[i] for i in order][::-1], importances[order][::-1])\n",
    "    plt.xlabel(\"Importance\")\n",
    "    plt.title(\"Top 10 Features (Classification RF)\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aac0452",
   "metadata": {},
   "source": [
    "\n",
    "## 8) Launch, Monitor, and Maintain — Production Approach\n",
    "\n",
    "**How to ship this at WCF:**\n",
    "1. Save the tuned pipeline using `joblib` (it includes preprocessing + model).\n",
    "2. Expose a prediction endpoint or batch job to score new claims.\n",
    "3. Store predictions with timestamps; route high-risk claims for human review.\n",
    "4. Monitor model drift and performance; retrain monthly/quarterly.\n",
    "5. Add governance: access controls, audit logs, bias monitoring by demographic segments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0aec8a5",
   "metadata": {},
   "source": [
    "\n",
    "## **Extra Analysis Ideas**\n",
    "\n",
    "In this section, we go beyond the basic regression/classification models and explore additional analytical techniques:\n",
    "\n",
    "1. **Clustering:** Segment claims based on injury description keyword embeddings to find patterns in claim types and costs.\n",
    "2. **Time-Series Trend Analysis:** Explore how claim costs and frequency evolve over time.\n",
    "3. **Scenario Simulation:** Estimate potential savings if the reporting delay is reduced.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9458dd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Select the claim description keyword columns\n",
    "keyword_cols = [col for col in df.columns if col.startswith(\"ClaimDescriptionKeyword_\")]\n",
    "X_keywords = df[keyword_cols]\n",
    "\n",
    "# Run KMeans clustering\n",
    "kmeans = KMeans(n_clusters=4, random_state=42)\n",
    "df['ClaimCluster'] = kmeans.fit_predict(X_keywords)\n",
    "\n",
    "# Cluster analysis: average cost per cluster\n",
    "cluster_costs = df.groupby('ClaimCluster')['UltimateIncurredClaimCost'].mean()\n",
    "print(\"Average cost per cluster:\\n\", cluster_costs)\n",
    "\n",
    "# Visualize clusters using first two PCA components\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "components = pca.fit_transform(X_keywords)\n",
    "\n",
    "plt.scatter(components[:,0], components[:,1], c=df['ClaimCluster'], cmap='viridis', alpha=0.6)\n",
    "plt.xlabel('PCA 1')\n",
    "plt.ylabel('PCA 2')\n",
    "plt.title('Claim Clusters (based on keywords)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee1a164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time-series: average cost per accident year\n",
    "df['AccidentYear'] = df['DateTimeOfAccident'].dt.year\n",
    "yearly_costs = df.groupby('AccidentYear')['UltimateIncurredClaimCost'].mean()\n",
    "\n",
    "yearly_costs.plot(marker='o')\n",
    "plt.ylabel('Average Claim Cost')\n",
    "plt.title('Average Claim Cost by Accident Year')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Claim frequency per year\n",
    "yearly_counts = df.groupby('AccidentYear').size()\n",
    "yearly_counts.plot(marker='s', color='orange')\n",
    "plt.ylabel('Number of Claims')\n",
    "plt.title('Number of Claims by Accident Year')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f773484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario: Reduce DaysToReport to <= 7 days for all claims\n",
    "df['DaysToReport'] = (df['DateReported'] - df['DateTimeOfAccident']).dt.days\n",
    "baseline_avg_cost = df['UltimateIncurredClaimCost'].mean()\n",
    "\n",
    "# Assume that for late-reported claims (>7 days), cost could be reduced by 5%\n",
    "df['SimulatedCost'] = df.apply(lambda row: row['UltimateIncurredClaimCost'] * 0.95 if row['DaysToReport'] > 7 else row['UltimateIncurredClaimCost'], axis=1)\n",
    "\n",
    "simulated_avg_cost = df['SimulatedCost'].mean()\n",
    "savings = baseline_avg_cost - simulated_avg_cost\n",
    "\n",
    "print(f\"Baseline avg cost: {baseline_avg_cost:,.2f}\")\n",
    "print(f\"Simulated avg cost (report <= 7 days): {simulated_avg_cost:,.2f}\")\n",
    "print(f\"Estimated savings per claim: {savings:,.2f}\")\n",
    "print(f\"Estimated annual savings (assuming {len(df)} claims/year): {savings * len(df):,.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
