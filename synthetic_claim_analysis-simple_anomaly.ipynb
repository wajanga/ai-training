{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42844d7c",
   "metadata": {},
   "source": [
    "\n",
    "# WCF AI Hands-On Lab: Claims Analytics & Fraud Detection\n",
    "\n",
    "This Colab notebook is designed for a 1‑day training with the Workers Compensation Fund (WCF) tech team.\n",
    "You'll work with an anonymized **WCF‑style sample dataset** and (optionally) the Kaggle **“Easy Peasy – Predict Worker Compensation Claims”** dataset.\n",
    "\n",
    "**Objectives**\n",
    "- Explore, clean, and visualize claims data.\n",
    "- Detect anomalies (potential fraud) using an unsupervised model.\n",
    "- Summarize injury text fields.\n",
    "- (Optional) Load and compare with the Kaggle *Easy Peasy* dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883646e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#@title Install and import libraries\n",
    "#!pip -q install pandas scikit-learn matplotlib nltk kaggle --upgrade\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import nltk\n",
    "import os\n",
    "nltk.download('punkt')\n",
    "print(\"Setup complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed7fa69",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Load the WCF-style Sample Dataset\n",
    "\n",
    "**Option A (Upload):** Download the CSV from the training resources, then upload here.  \n",
    "**Option B (Google Drive):** Place the CSV in your Drive and load via path.\n",
    "\n",
    "> File name: `wcf_sample_claims.csv`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e2040d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try local dataset path first (relative to the notebook's working directory)\n",
    "local_dataset_dir = os.path.join(os.getcwd(), \"datasets\")\n",
    "local_csv_path = os.path.join(local_dataset_dir, \"synthetic_sample_claims.csv\")\n",
    "    \n",
    "# Fallback: absolute path to the repo dataset folder (if running from a subdir)\n",
    "repo_root = \"/Users/aronkondoro/Library/Mobile Documents/com~apple~CloudDocs/Projects/WCF\"\n",
    "fallback_csv_path = os.path.join(repo_root, \"dataset\", \"synthetic_sample_claims.csv\")\n",
    "\n",
    "csv_path = local_csv_path if os.path.exists(local_csv_path) else fallback_csv_path\n",
    "print(f\"Loading CSV from: {csv_path}\")\n",
    "\n",
    "df = pd.read_csv(csv_path, parse_dates=[\"Date_Filed\"]) \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5873de",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Quick Exploration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a529551",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.info()\n",
    "display(df.describe(include='all'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533b6f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['Suspected_Fraud'].value_counts(normalize=True).rename('share').to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb3fff0",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Visualize Claims\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b167cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure()\n",
    "plt.scatter(df['Claim_Amount_TZS'], df['Processing_Time_Days'])\n",
    "plt.xlabel(\"Claim Amount (TZS)\")\n",
    "plt.ylabel(\"Processing Time (days)\")\n",
    "plt.title(\"Claims: Amount vs Processing Time\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faef1ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure()\n",
    "df.groupby('Region')['Claim_Amount_TZS'].mean().sort_values().plot(kind='bar')\n",
    "plt.title(\"Average Claim Amount by Region\")\n",
    "plt.ylabel(\"TZS\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e89f00",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Anomaly Detection (Unsupervised)\n",
    "We'll use **IsolationForest** on numerical features to flag potentially unusual claims.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3416b046",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "features = df[['Claim_Amount_TZS', 'Processing_Time_Days', 'Age']].copy()\n",
    "model = IsolationForest(contamination=0.05, random_state=42)\n",
    "df['anomaly'] = model.fit_predict(features)\n",
    "\n",
    "outliers = df[df['anomaly'] == -1]\n",
    "print(f\"Flagged {len(outliers)} / {len(df)} claims as unusual (~{len(outliers)/len(df):.1%}).\")\n",
    "outliers[['Claim_ID','Claim_Amount_TZS','Processing_Time_Days','Sector','Channel','Suspected_Fraud']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e8515f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(df['Claim_Amount_TZS'], df['Processing_Time_Days'], c=(df['anomaly']==-1).astype(int))\n",
    "plt.xlabel(\"Claim Amount (TZS)\")\n",
    "plt.ylabel(\"Processing Time (days)\")\n",
    "plt.title(\"Outliers Highlighted\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8220f60",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Quick Text Summaries (NLP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0b118a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def summarize_text(text):\n",
    "    sents = sent_tokenize(text or \"\")\n",
    "    return sents[0] if sents else \"\"\n",
    "\n",
    "df['Summary'] = df['Injury_Description'].apply(summarize_text)\n",
    "df[['Claim_ID','Injury_Description','Summary']].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb089d5f",
   "metadata": {},
   "source": [
    "\n",
    "## 6) (Optional) Load the Kaggle **“Easy Peasy – Predict Worker Compensation Claims”** Dataset\n",
    "\n",
    "**One-time setup in Colab:**\n",
    "1. Create a Kaggle account, then go to *Account → API → Create New Token*. This downloads `kaggle.json`.\n",
    "2. Upload `kaggle.json` to Colab (left sidebar → files).\n",
    "3. Run the cell below to place it at `~/.kaggle/kaggle.json` and download the dataset.\n",
    "\n",
    "> Dataset slug: `lucamassaron/easy-peasy-its-lemon-squeezy`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fb5100",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#@title Kaggle API setup (run after uploading kaggle.json)\n",
    "import os, shutil, zipfile, glob\n",
    "\n",
    "os.makedirs(os.path.expanduser(\"~/.kaggle\"), exist_ok=True)\n",
    "if os.path.exists(\"/content/kaggle.json\"):\n",
    "    shutil.move(\"/content/kaggle.json\", os.path.expanduser(\"~/.kaggle/kaggle.json\"))\n",
    "os.chmod(os.path.expanduser(\"~/.kaggle/kaggle.json\"), 0o600)\n",
    "\n",
    "!kaggle datasets download -d lucamassaron/easy-peasy-its-lemon-squeezy -p /content/easy_peasy -q\n",
    "# Unzip\n",
    "for z in glob.glob(\"/content/easy_peasy/*.zip\"):\n",
    "    with zipfile.ZipFile(z, 'r') as zip_ref:\n",
    "        zip_ref.extractall(\"/content/easy_peasy\")\n",
    "print(\"Easy Peasy dataset ready at /content/easy_peasy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6718c89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# List files to identify CSVs to load\n",
    "import os\n",
    "\n",
    "for root, dirs, files in os.walk(\"/content/easy_peasy\"):\n",
    "    for f in files:\n",
    "        if f.lower().endswith(\".csv\"):\n",
    "            print(os.path.join(root, f))\n",
    "\n",
    "# Example usage:\n",
    "# easy_df = pd.read_csv(\"/content/easy_peasy/<replace_with_csv_name>.csv\")\n",
    "# easy_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa20857",
   "metadata": {},
   "source": [
    "\n",
    "## 7) Next Steps\n",
    "- Discuss which additional fields (e.g., diagnosis codes, employer compliance history) would improve detection.\n",
    "- Draft a WCF pilot plan: data access, success metrics, integration points, and governance.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
